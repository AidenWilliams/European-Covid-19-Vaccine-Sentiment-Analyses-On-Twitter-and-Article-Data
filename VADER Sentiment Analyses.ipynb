{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Sentiment Analyses using VADER on Twitter Data\n",
    "\n",
    "Using the processed data saved from the Get Tweets notebook, here the VADER sentiment analyzer is used to get the compound\n",
    "sentiment analyses for each tweet. This compound score is added to the Tweet json files and are stored in the Data/Analyzed\n",
    "folder.\n",
    "\n",
    "The mean sentiment is also stored as a json file in the Data folder."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to C:\\Users\\Aiden\n",
      "[nltk_data]     Williams\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
    "import nltk\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "languages = {\n",
    "                1: 'en',\n",
    "                2: 'es',\n",
    "                3: 'fr',\n",
    "                4: 'de',\n",
    "                5: 'nl',\n",
    "                6: 'it',\n",
    "            }\n",
    "\n",
    "months = ['December', 'January', 'February', 'March', 'April', 'May']"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the below cell we loop for each tweet text file and get the compound sentiment score. This extra feature is added to\n",
    "the dataframe before being saved in the Data/Analyzed folder."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/6 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "aa09ac7248c1469db79f99877fb1e5d7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-4-287ec5d24b02>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      8\u001B[0m             \u001B[1;32mfor\u001B[0m \u001B[0mtext\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mtweetsP\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'text'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      9\u001B[0m                 \u001B[1;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtext\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstr\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 10\u001B[1;33m                     \u001B[0mpol_score\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mSIA\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpolarity_scores\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtext\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;31m# run analysis\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     11\u001B[0m                     \u001B[0mpol_score\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'text'\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtext\u001B[0m \u001B[1;31m# add headlines for viewing\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     12\u001B[0m                     \u001B[0mresults\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpol_score\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\Anaconda\\envs\\IAPT\\lib\\site-packages\\nltk\\sentiment\\vader.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, lexicon_file)\u001B[0m\n\u001B[0;32m    338\u001B[0m     ):\n\u001B[0;32m    339\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlexicon_file\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnltk\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mload\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlexicon_file\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 340\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlexicon\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmake_lex_dict\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    341\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mconstants\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mVaderConstants\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    342\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\Anaconda\\envs\\IAPT\\lib\\site-packages\\nltk\\sentiment\\vader.py\u001B[0m in \u001B[0;36mmake_lex_dict\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    348\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mline\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlexicon_file\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msplit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"\\n\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    349\u001B[0m             \u001B[1;33m(\u001B[0m\u001B[0mword\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmeasure\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mline\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstrip\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msplit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"\\t\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 350\u001B[1;33m             \u001B[0mlex_dict\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mword\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mfloat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmeasure\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    351\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mlex_dict\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    352\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "Language = defaultdict(lambda: [])\n",
    "for month in tqdm(months):\n",
    "    for day in [0, 1, 2, 3, 4]:\n",
    "        for language in languages:\n",
    "            path = 'Data/Text/' + str(month) + str(day) + languages[language] + '.json'\n",
    "            tweetsP = pd.read_json(path).T\n",
    "            results = []\n",
    "            for text in tweetsP['text']:\n",
    "                if isinstance(text, str):\n",
    "                    pol_score = SIA().polarity_scores(text) # run analysis\n",
    "                    pol_score['text'] = text # add headlines for viewing\n",
    "                    results.append(pol_score)\n",
    "\n",
    "            tweetsP['Score'] = pd.DataFrame(results)['compound']\n",
    "            Language[language].append(np.average(tweetsP['Score']))\n",
    "            tweetsP.to_json('Data/Analyzed Tweets/' + str(month) + str(day) + languages[language] + '.json')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "numpy sometimes returns a NAN, here this is checked and replaced with a 0, equivalent to a true neutral score."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for l in Language:\n",
    "    _curr_l = Language[l]\n",
    "    curr_l = []\n",
    "    for mean in _curr_l:\n",
    "        if np.isnan(mean):\n",
    "            curr_l.append(0)\n",
    "        else:\n",
    "            curr_l.append(mean)\n",
    "    Language[l] = curr_l"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The mean sentiment is stored in this format:\n",
    "\n",
    "{\n",
    "\n",
    "Language 0 : {Month 0: [Day 0 ... Day 29] ... Month 5: [Day 0 ... Day 29]}\n",
    ".\n",
    ".\n",
    ".\n",
    "Language 5 : {Month 0: [Day 0 ... Day 29] ... Month 5: [Day 0 ... Day 29]}\n",
    "\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "to_save = {}\n",
    "for i, month in enumerate(Language):\n",
    "    to_save[i] = {'month': month, 'day': Language[month]}\n",
    "json.dump(to_save, open('Data/MeanSentiment.json', 'w+'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Sentiment Score using VADER on the Article Headings Dataset\n",
    "\n",
    "Another dataset gathered is the article dataset. 5 articles for each day (1, 7, 14, 21, 28) were collected for each\n",
    "language's european mother country, .i.e. English = UK, French = France.\n",
    "\n",
    "This dataset set was collected manually using the Google search engine using the following query:\n",
    "\n",
    "\n",
    "```{Country} Covid* before:{Date in YYYY-MM-DD Format} After:{Day before Date in YYYY-MM-DD Format} ```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The Article Headings dataset consists of 5 csv files, with 30 dates corresponding to the twitter dataset dates. Each date\n",
    "has 5 articles attached to it in separate columns. For this reason each row in the dataframe has a loop run to get the\n",
    "sentiment score for each article heading. The mean score is collected at this stage as well."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/6 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "11b410471372453ba769db2963a10916"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "root = 'Data/Articles'\n",
    "if not os.access(root, os.R_OK):\n",
    "    print(\"Check dataroot!!\")\n",
    "\n",
    "mean_score = defaultdict(lambda: [])\n",
    "for country in tqdm(os.listdir(root)):\n",
    "    file_path = os.path.join(root, country)\n",
    "    articles = pd.read_csv(file_path)\n",
    "    new_dataframe = pd.DataFrame(columns=['Date', 'Article', 'Score'])\n",
    "\n",
    "    i = 0\n",
    "    while i < articles.shape[0]:\n",
    "        _articles = []\n",
    "        for j in range(1, 6):\n",
    "            _articles.append(SIA().polarity_scores(articles['Article ' + str(j)][i])['compound'])\n",
    "            new_dataframe = new_dataframe.append(\n",
    "                            {\n",
    "                                'Date': articles['Date'][i],\n",
    "                                'Article': articles['Article ' + str(j)][i],\n",
    "                                'Score': _articles[-1]\n",
    "                            },\n",
    "                            ignore_index=True)\n",
    "        mean_score[country[:-4]].append(np.average(_articles))\n",
    "        i += 1\n",
    "\n",
    "    new_dataframe.to_json('Data/Analyzed Articles/Article Score ' + str(country[:-4]) + '.json')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Check for NaN items then save the mean score in json."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for c in mean_score:\n",
    "    _curr_c = mean_score[c]\n",
    "    curr_c = []\n",
    "    for mean in _curr_c:\n",
    "        if np.isnan(mean):\n",
    "            curr_c.append(0)\n",
    "        else:\n",
    "            curr_c.append(mean)\n",
    "    mean_score[c] = curr_c\n",
    "\n",
    "to_save = {}\n",
    "for i, country in enumerate(mean_score):\n",
    "    to_save[os.listdir(root)[i][:-4]] = {'country': country, 'day': mean_score[country]}\n",
    "json.dump(to_save, open('Data/ArticleMeanSentiment.json', 'w+'))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}